{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "438b9d10-0b61-45f8-bfeb-884b592e50c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6bdef0d-8dda-42b2-812b-122f59c6b869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shko8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\shko8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shko8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shko8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\shko8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('webtext')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca0af524-3ec8-4365-a9b6-d2a0a35ba490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone.', \"It's good to see you.\", \"Let's start our text mining class!\"]\n"
     ]
    }
   ],
   "source": [
    "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "print(sent_tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acd4ef99-2813-46cd-b81d-c68ffb37eb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Je t'ai demandé si tu m'aimais bien, Tu m'a répondu non.\", \"Je t'ai demandé si j'étais jolie, Tu m'a répondu non.\", \"Je t'ai demandé si j'étai dans ton coeur, Tu m'a répondu non.\"]\n"
     ]
    }
   ],
   "source": [
    "paragraph_french = \"\"\"Je t'ai demandé si tu m'aimais bien, Tu m'a répondu non. \n",
    "Je t'ai demandé si j'étais jolie, Tu m'a répondu non. \n",
    "Je t'ai demandé si j'étai dans ton coeur, Tu m'a répondu non.\"\"\"\n",
    "\n",
    "import nltk\n",
    "# nltk.data.path.append('C:/Users/shko8/AppData/Roaming/nltk_data')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "print(tokenizer.tokenize(paragraph_french))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e51044b2-ef38-466d-a681-dcd0a3daa01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요, 여러분.', '만나서 반갑습니다.', '이제 텍스트마이닝 클래스를 시작해봅시다!']\n"
     ]
    }
   ],
   "source": [
    "para_kor = \"안녕하세요, 여러분. 만나서 반갑습니다. 이제 텍스트마이닝 클래스를 시작해봅시다!\"\n",
    "print(sent_tokenize(para_kor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "928d31d4-28d3-4d9d-8b08-feebb0fff176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13ead45f-70fb-404b-b88d-8807cf298576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'\", 's', 'good', 'to', 'see', 'you', '.', 'Let', \"'\", 's', 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(WordPunctTokenizer().tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98c975b4-465b-4e84-9777-74c761d26221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요', ',', '여러분', '.', '만나서', '반갑습니다', '.', '이제', '텍스트마이닝', '클래스를', '시작해봅시다', '!']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(para_kor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee70511f-8946-4ae4-8821-b13d74aacffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.findall(\"[abc]\", 'how are you, boy?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5eb3819-abe9-4d20-9bc0-44ed5ded6b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '7', '5', '9']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[0123456789]','3a7b5c9d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b8fe688-9804-4ece-96e0-e7eda394ab02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', 'a', '7', 'b', '_', '5', 'c', '9', 'd']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[\\w]\", \"3a 7b_ ',^&5c9d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07b67e0f-1ff8-4fe6-bfbf-e84e9ae61b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_', '__', '__']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[_]+\",\"a_b, c__d, e__f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "139e6d4f-6c56-4b35-a55b-d5a7675d79ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how', 'are', 'you', 'boy']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[\\w]+\",'how are you, boy?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e51434e-db82-4750-b92d-0541b975e4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oo', 'ooo', 'oooo', 'ooo']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[o]{2,4}\", \"oh, hoow are yooou, boooooooy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5461c327-e872-43cd-9181-881e37a49c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', 'I', \"can't\", 'go', 'there']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "print(tokenizer.tokenize(\"sorry, I can't go there\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e62ba74a-daa7-4a1a-9088-66b073326a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', 'I', 'can', 't', 'go', 'there']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
    "print(tokenizer.tokenize(\"sorry, I can't go there\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12710fc9-e440-4a66-b431-0be4511727db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', \"can't\", 'there']\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Sorry, I can't go there\"\n",
    "tokenizer = RegexpTokenizer(\"[\\w']{3,}\")\n",
    "print(tokenizer.tokenize(text1.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0495c5e6-4601-469e-966f-29a2c0440623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', 'go', 'movie', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "text1 = \"Sorry, I couldn't go to movie yesterday\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokens = tokenizer.tokenize(text1.lower())\n",
    "\n",
    "result = [word for word in tokens if word not in english_stops]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1bff2ae6-8742-4567-ae9c-8043f0251887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"you've\", 'in', 'what', 'from', 'your', 'had', 'very', 'only', 'doesn', 'that', 'at', 'her', 'through', 'their', 'with', 've', 'not', 'own', 'there', 'couldn', 'each', 'hers', \"haven't\", 'against', 'out', 'such', 'yourselves', 'when', 's', 'now', 'where', 'how', 'after', 'them', \"needn't\", \"won't\", 'herself', 'do', 'all', 'here', 'further', \"mustn't\", 'are', 'was', 'themselves', 'isn', 'be', 'until', 'up', 'will', 'wouldn', \"couldn't\", 'don', 'below', 'were', 'needn', 'aren', 'its', 'hasn', 'you', 'any', 'why', 'they', 'those', \"you'd\", 'and', 'wasn', 'under', 'about', 'once', 'he', 'a', 'same', \"doesn't\", 'him', \"shouldn't\", 'am', \"hadn't\", 'shan', \"wasn't\", 'of', 'whom', 'doing', 'before', 'most', 't', 'it', 're', 'again', 'just', 'been', 'which', 'm', 'into', \"aren't\", 'ours', 'between', 'won', 'y', 'mustn', \"weren't\", 'my', 'on', 'so', 'should', 'as', 'to', 'ourselves', 'too', \"it's\", 'o', 'having', 'who', 'during', 'yours', 'for', 'we', 'his', 'than', 'both', \"don't\", \"you're\", 'some', \"mightn't\", 'other', 'but', 'off', \"she's\", 'is', 'more', 'the', 'then', 'few', 'over', 'itself', 'ain', 'll', 'have', 'can', 'being', 'haven', 'i', 'myself', \"that'll\", \"didn't\", 'theirs', 'an', 'if', \"shan't\", 'ma', 'shouldn', 'by', 'weren', 'these', 'because', \"hasn't\", \"you'll\", 'himself', 'd', 'yourself', \"wouldn't\", 'mightn', \"should've\", 'does', 'she', 'while', 'did', 'nor', 'has', 'no', 'hadn', 'above', \"isn't\", 'our', 'didn', 'or', 'me', 'this', 'down'}\n"
     ]
    }
   ],
   "source": [
    "print(english_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "98f16e81-1e06-42fd-a658-8777b8b79cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopword = ['i', 'go', 'to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "767bd8e0-f646-4d4d-a979-f6a77826e3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', \"couldn't\", 'movie', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "result = [word for word in tokens if word not in my_stopword]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "937e1880-60e1-4608-9409-3dc59e04faa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookeri cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3c08b45d-7bd6-4d32-a3c9-20da5722d2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n",
      "['hello', 'everyon', '.', 'it', \"'s\", 'good', 'to', 'see', 'you', '.', 'let', \"'s\", 'start', 'our', 'text', 'mine', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
    "tokens = word_tokenize(para)\n",
    "print(tokens)\n",
    "result= [stemmer.stem(token) for token in tokens]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d0a8ba26-10d3-48fd-979d-187e1d97336c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookery cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c39504c-c23b-4270-8f7b-13b6aee60955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking\n",
      "cook\n",
      "cookery\n",
      "cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('cooking'))\n",
    "print(lemmatizer.lemmatize('cooking', pos='v'))\n",
    "print(lemmatizer.lemmatize('cookery'))\n",
    "print(lemmatizer.lemmatize('cookbooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3739fa21-62c8-47eb-a359-5c93e22a19a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming result:  believ\n",
      "lemmatizing result:  belief\n",
      "lemmatizing result:  believe\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "print('stemming result: ', stemmer.stem('believes'))\n",
    "print('lemmatizing result: ', lemmatizer.lemmatize('believes'))\n",
    "print('lemmatizing result: ', lemmatizer.lemmatize('believes', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "03ebdaa4-a537-4a7e-9032-5b47d410a7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('everyone', 'NN'), ('.', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('good', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('.', '.'), ('Let', 'VB'), (\"'s\", 'POS'), ('start', 'VB'), ('our', 'PRP$'), ('text', 'NN'), ('mining', 'NN'), ('class', 'NN'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(\"Hello everyone. It's good to see you. Let's start our text mining class!\")\n",
    "print(nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "70a85461-5d7c-4b30-87f9-2b23da5f5502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('CC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bd642a9d-ace4-4404-a8eb-19a24441e71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['everyone', 'good', 'see', 'Let', 'start', 'text', 'mining', 'class']\n"
     ]
    }
   ],
   "source": [
    "my_tag_set = ['NN', 'VB', 'JJ']\n",
    "my_words = [word for word, tag in nltk.pos_tag(tokens) if tag in my_tag_set]\n",
    "print(my_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0108b993-3e4f-4700-817f-ca8dbd237c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello/NNP', 'everyone/NN', './.', 'It/PRP', \"'s/VBZ\", 'good/JJ', 'to/TO', 'see/VB', 'you/PRP', './.', 'Let/VB', \"'s/POS\", 'start/VB', 'our/PRP$', 'text/NN', 'mining/NN', 'class/NN', '!/.']\n"
     ]
    }
   ],
   "source": [
    "words_with_tag = ['/'.join(item) for item in nltk.pos_tag(tokens)]\n",
    "print(words_with_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9eaafcdc-2011-47b1-b9f8-a6dab8826397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['절망의', '반대가', '희망은', '아니다', '.', '어두운', '밤하늘에', '별이', '빛나듯', '희망은', '절망', '속에', '싹트는', '거지', '만약에', '우리가', '희망함이', '적다면', '그', '누가', '세상을', '비출어줄까', '.', '정희성', ',', '희망', '공부']\n",
      "[('절망의', 'JJ'), ('반대가', 'NNP'), ('희망은', 'NNP'), ('아니다', 'NNP'), ('.', '.'), ('어두운', 'VB'), ('밤하늘에', 'JJ'), ('별이', 'NNP'), ('빛나듯', 'NNP'), ('희망은', 'NNP'), ('절망', 'NNP'), ('속에', 'NNP'), ('싹트는', 'NNP'), ('거지', 'NNP'), ('만약에', 'NNP'), ('우리가', 'NNP'), ('희망함이', 'NNP'), ('적다면', 'NNP'), ('그', 'NNP'), ('누가', 'NNP'), ('세상을', 'NNP'), ('비출어줄까', 'NNP'), ('.', '.'), ('정희성', 'NN'), (',', ','), ('희망', 'NNP'), ('공부', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "sentence = '''절망의 반대가 희망은 아니다.\n",
    "어두운 밤하늘에 별이 빛나듯\n",
    "희망은 절망 속에 싹트는 거지\n",
    "만약에 우리가 희망함이 적다면\n",
    "그 누가 세상을 비출어줄까.\n",
    "정희성, 희망 공부'''\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n",
    "print(nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1673bc23-84ca-4861-97ef-9ec0cf5bde6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from konlpy) (1.5.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from konlpy) (5.3.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from konlpy) (1.24.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed014f2-dea2-4650-8a79-22efb2fdbcf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "JVMNotFoundException",
     "evalue": "No JVM shared library file (jvm.dll) found. Try setting up the JAVA_HOME environment variable properly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJVMNotFoundException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkonlpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Okt\n\u001b[1;32m----> 2\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mOkt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3_8_5\\lib\\site-packages\\konlpy\\tag\\_okt.py:51\u001b[0m, in \u001b[0;36mOkt.__init__\u001b[1;34m(self, jvmpath, max_heap_size)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, jvmpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_heap_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jpype\u001b[38;5;241m.\u001b[39misJVMStarted():\n\u001b[1;32m---> 51\u001b[0m         \u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjvmpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_heap_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     oktJavaPackage \u001b[38;5;241m=\u001b[39m jpype\u001b[38;5;241m.\u001b[39mJPackage(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkr.lucypark.okt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     54\u001b[0m     OktInterfaceJavaClass \u001b[38;5;241m=\u001b[39m oktJavaPackage\u001b[38;5;241m.\u001b[39mOktInterface\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3_8_5\\lib\\site-packages\\konlpy\\jvm.py:55\u001b[0m, in \u001b[0;36minit_jvm\u001b[1;34m(jvmpath, max_heap_size)\u001b[0m\n\u001b[0;32m     52\u001b[0m args \u001b[38;5;241m=\u001b[39m [javadir, os\u001b[38;5;241m.\u001b[39msep]\n\u001b[0;32m     53\u001b[0m classpath \u001b[38;5;241m=\u001b[39m [f\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m folder_suffix]\n\u001b[1;32m---> 55\u001b[0m jvmpath \u001b[38;5;241m=\u001b[39m jvmpath \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetDefaultJVMPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# NOTE: Temporary patch for Issue #76. Erase when possible.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdarwin\u001b[39m\u001b[38;5;124m'\u001b[39m\\\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m jvmpath\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1.8.0\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\\\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m jvmpath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlibjvm.dylib\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3_8_5\\lib\\site-packages\\jpype\\_jvmfinder.py:74\u001b[0m, in \u001b[0;36mgetDefaultJVMPath\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     finder \u001b[38;5;241m=\u001b[39m LinuxJVMFinder()\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_jvm_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3_8_5\\lib\\site-packages\\jpype\\_jvmfinder.py:212\u001b[0m, in \u001b[0;36mJVMFinder.get_jvm_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jvm_notsupport_ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m jvm_notsupport_ext\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m JVMNotFoundException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo JVM shared library file (\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    213\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound. Try setting up the JAVA_HOME \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    214\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment variable properly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    215\u001b[0m                            \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libfile))\n",
      "\u001b[1;31mJVMNotFoundException\u001b[0m: No JVM shared library file (jvm.dll) found. Try setting up the JAVA_HOME environment variable properly."
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "t = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd14cf-8303-41f8-aea5-039c9c725de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5decc126-b0d5-4462-bd79-3215abfb91b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03576738-864d-42cb-9b7d-7a4cc4bb1c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdf41c1-235b-40f5-9d14-25394e99265c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf57226d-88bc-44b3-b72f-a041e0f1e437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_8_5",
   "language": "python",
   "name": "py3_8_5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
