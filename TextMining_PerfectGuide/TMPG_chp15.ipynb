{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f53fd7e-7a3e-49ab-ae00-46a8e91c84f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a beautiful day! 토큰화결과: ['what', 'a', 'beautiful', 'day', '!']\n",
      "Nvidia Titan XP has 12GB of VRAM 토큰화결과: ['n', '##vid', '##ia', 'titan', 'xp', 'has', '12', '##gb', 'of', 'vr', '##am']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "sentence1 = 'What a beautiful day!'\n",
    "sentence2 = 'Nvidia Titan XP has 12GB of VRAM'\n",
    "\n",
    "print(sentence1, '토큰화결과:', tokenizer.tokenize(sentence1))\n",
    "print(sentence2, '토큰화결과:', tokenizer.tokenize(sentence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a2dd5bb-74cc-47fd-9b26-8a98641d8c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT입력: {'input_ids': [[101, 2054, 1037, 3376, 2154, 999, 102, 0, 0, 0, 0, 0, 0], [101, 1050, 17258, 2401, 16537, 26726, 2038, 2260, 18259, 1997, 27830, 3286, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer([sentence1, sentence2], padding=True)\n",
    "print('BERT입력:', inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa254b23-9f9b-4264-b07f-55b026283c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "두 문장 시퀀스에 대한 BERT입력: {'input_ids': [101, 2054, 1037, 3376, 2154, 999, 102, 1050, 17258, 2401, 16537, 26726, 2038, 2260, 18259, 1997, 27830, 3286, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(sentence1, sentence2, padding=True)\n",
    "print('두 문장 시퀀스에 대한 BERT입력:', inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc7f0c1f-344d-453d-9a35-bbef0ebc3e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:/Users/shko8/AppData/Roaming/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set count:  1600\n",
      "Test set count:  400\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "fileids = movie_reviews.fileids()\n",
    "\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in fileids]\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids]\n",
    "\n",
    "label_dict = {'pos':1, 'neg':0}\n",
    "y = [label_dict[c] for c in categories]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    reviews, y, test_size=0.2, random_state=7\n",
    ")\n",
    "\n",
    "print('Train set count: ', len(X_train))\n",
    "print('Test set count: ', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ee56905-0f61-4197-8221-7da5a10401c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_input = tokenizer(X_train, truncation=True, padding=True, return_tensors='pt')\n",
    "test_input = tokenizer(X_test, truncation=True, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9dcb6c8-0b05-468f-9a53-821b910a3c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp38-cp38-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp38-cp38-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py38-none-any.whl.metadata (7.1 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.10.10-cp38-cp38-win_amd64.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp38-cp38-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp38-cp38-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.15.2-cp38-cp38-win_amd64.whl.metadata (58 kB)\n",
      "     ---------------------------------------- 0.0/58.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 58.1/58.1 kB ? eta 0:00:00\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets)\n",
      "  Downloading propcache-0.2.0-cp38-cp38-win_amd64.whl.metadata (7.9 kB)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "   ---------------------------------------- 0.0/480.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 480.6/480.6 kB 15.2 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.3/116.3 kB 7.1 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "   ---------------------------------------- 0.0/179.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 179.3/179.3 kB 10.6 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.10.10-cp38-cp38-win_amd64.whl (383 kB)\n",
      "   ---------------------------------------- 0.0/383.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 383.5/383.5 kB 12.0 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py38-none-any.whl (132 kB)\n",
      "   ---------------------------------------- 0.0/132.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 132.6/132.6 kB 7.6 MB/s eta 0:00:00\n",
      "Downloading pyarrow-17.0.0-cp38-cp38-win_amd64.whl (25.2 MB)\n",
      "   ---------------------------------------- 0.0/25.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.5/25.2 MB 31.8 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 5.0/25.2 MB 53.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 7.6/25.2 MB 48.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 8.8/25.2 MB 43.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 11.4/25.2 MB 46.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 14.1/25.2 MB 46.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 16.8/25.2 MB 46.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.5/25.2 MB 59.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.1/25.2 MB 59.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/25.2 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.2/25.2 MB 50.1 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp38-cp38-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.5.0-cp38-cp38-win_amd64.whl (51 kB)\n",
      "   ---------------------------------------- 0.0/51.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 51.9/51.9 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading multidict-6.1.0-cp38-cp38-win_amd64.whl (28 kB)\n",
      "Downloading yarl-1.15.2-cp38-cp38-win_amd64.whl (84 kB)\n",
      "   ---------------------------------------- 0.0/85.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 85.0/85.0 kB ? eta 0:00:00\n",
      "Downloading propcache-0.2.0-cp38-cp38-win_amd64.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.7/45.7 kB ? eta 0:00:00\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "Successfully installed aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 async-timeout-4.0.3 datasets-3.1.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.9.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.0 pyarrow-17.0.0 xxhash-3.5.0 yarl-1.15.2\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "075dc91d-0da8-4659-aa5c-bc8090be5029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = OurDataset(train_input, y_train)\n",
    "test_dataset = OurDataset(test_input, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e509e8c-8d84-495b-aff0-d909480aa2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install evaluate\n",
    "# from datasets import load_metric\n",
    "import evaluate\n",
    "\n",
    "# metric = load_metric('accuracy')\n",
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80c15857-571e-460c-9300-2903b20da3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2762b096-fb2a-4532-b6e3-31afefbf8453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from transformers[torch]) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from transformers[torch]) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from transformers[torch]) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from transformers[torch]) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from transformers[torch]) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from transformers[torch]) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from transformers[torch]) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from transformers[torch]) (1.0.1)\n",
      "Requirement already satisfied: torch in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from transformers[torch]) (2.4.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from torch->transformers[torch]) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from torch->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from requests->transformers[torch]) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from requests->transformers[torch]) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f391a92d-bc57-4d96-bd40-cf58f811bc06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_28452\\2949566832.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 50:28, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=0.4781996536254883, metrics={'train_runtime': 3036.4244, 'train_samples_per_second': 1.054, 'train_steps_per_second': 0.132, 'total_flos': 841955377152000.0, 'train_loss': 0.4781996536254883, 'epoch': 2.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ede3873-1877-4739-ad35-59b779a51f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_28452\\2949566832.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 01:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.32017433643341064,\n",
       " 'eval_accuracy': 0.88,\n",
       " 'eval_runtime': 107.0585,\n",
       " 'eval_samples_per_second': 3.736,\n",
       " 'eval_steps_per_second': 0.234,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ecc4eb-c050-4cc9-8b6f-353280b2b325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8354a860-56c1-439d-b644-1c842bfc7b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84a0bab0-bd5c-4e1e-b88f-3a970a3b1513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:/Users/shko8/AppData/Roaming/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set count:  1600\n",
      "Test set count:  400\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "fileids = movie_reviews.fileids()\n",
    "\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in fileids]\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids]\n",
    "\n",
    "label_dict = {'pos':1, 'neg':0}\n",
    "y = [label_dict[c] for c in categories]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    reviews, y, test_size=0.2, random_state=7\n",
    ")\n",
    "\n",
    "print('Train set count: ', len(X_train))\n",
    "print('Test set count: ', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbb9b986-d8a3-4704-8a9b-6d37cfc13eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b69e7967-42fb-4eac-bdfd-c66802c5747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "029f4c1f-25d4-4bdc-9211-b54d92cd4de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, pretrained_model, token_size, num_labels):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.token_size = token_size\n",
    "        self.num_labels = num_labels\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.classifier = torch.nn.Linear(self.token_size, self.num_labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.pretrained_model(**inputs)\n",
    "        bert_clf_token = outputs.last_hidden_state[:,0,:]\n",
    "        return self.classifier(bert_clf_token)\n",
    "\n",
    "model = MyModel(bert_model, num_labels=2, token_size=bert_model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9ea6a9e-f017-45f5-a054-6a834be57f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_28452\\2949566832.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 100, elapsed time: 763.44, loss: 0.7006\n",
      "Epoch 1, batch 200, elapsed time: 1527.76, loss: 0.5969\n",
      "Average loss for epoch 1: 0.7054\n",
      "Epoch 2, batch 100, elapsed time: 2292.48, loss: 0.1921\n",
      "Epoch 2, batch 200, elapsed time: 3096.06, loss: 0.1517\n",
      "Average loss for epoch 2: 0.5069\n",
      "Epoch 3, batch 100, elapsed time: 4194.98, loss: 0.1996\n",
      "Epoch 3, batch 200, elapsed time: 5275.01, loss: 0.0789\n",
      "Average loss for epoch 3: 0.2322\n",
      "Epoch 4, batch 100, elapsed time: 6349.20, loss: 0.0143\n",
      "Epoch 4, batch 200, elapsed time: 7504.56, loss: 0.0030\n",
      "Average loss for epoch 4: 0.0929\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "num_epochs = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_epoch_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        optim.zero_grad()\n",
    "    \n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(inputs)\n",
    "    \n",
    "        loss = criterion(outputs, F.one_hot(labels, num_classes=2).float())\n",
    "        if (step+1) % 100 == 0:\n",
    "            elapsed = time.time() - start\n",
    "            print(\n",
    "                'Epoch %d, batch %d, elapsed time: %.2f, loss: %.4f'\n",
    "                % (epoch+1, step+1, elapsed, loss)\n",
    "            )\n",
    "        total_epoch_loss += loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    avg_epoch_loss = total_epoch_loss / len(train_loader)\n",
    "    print('Average loss for epoch %d: %.4f' % (epoch+1, avg_epoch_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5e6bb27-472d-4ab1-843d-b400e04983c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_28452\\2949566832.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8425}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from datasets import load_metric\n",
    "import evaluate\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# metric = load_metric('accuracy')\n",
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "    labels = batch['labels'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        #print(outputs)\n",
    "\n",
    "    predictions = torch.argmax(outputs, dim=1)\n",
    "    metric.add_batch(predictions=predictions, references=labels)\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29cbde7-9a57-499d-9aa7-2124fd15237b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a986e-7a5d-453c-940d-4baa7aeee8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a07e8-2f00-49cf-b819-209fb27f4550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0190c47-2abf-4a69-89ca-fb043b7dca70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b58737-8c3d-4b9f-8349-5d5a1713c3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e835b-da95-4bb1-ac11-c08259dde3d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42287d0c-0012-4c7b-9530-bcf62ee3f3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83a9484-bd56-417f-9740-1c5c2a5c2418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9578262-7311-4d7b-a6d8-dea7e20dc6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13163808-a6cb-4408-9e7a-baeae367ce8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
