{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "438b9d10-0b61-45f8-bfeb-884b592e50c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6bdef0d-8dda-42b2-812b-122f59c6b869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shko8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\shko8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shko8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shko8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\shko8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('webtext')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca0af524-3ec8-4365-a9b6-d2a0a35ba490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone.', \"It's good to see you.\", \"Let's start our text mining class!\"]\n"
     ]
    }
   ],
   "source": [
    "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "print(sent_tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acd4ef99-2813-46cd-b81d-c68ffb37eb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Je t'ai demandé si tu m'aimais bien, Tu m'a répondu non.\", \"Je t'ai demandé si j'étais jolie, Tu m'a répondu non.\", \"Je t'ai demandé si j'étai dans ton coeur, Tu m'a répondu non.\"]\n"
     ]
    }
   ],
   "source": [
    "paragraph_french = \"\"\"Je t'ai demandé si tu m'aimais bien, Tu m'a répondu non. \n",
    "Je t'ai demandé si j'étais jolie, Tu m'a répondu non. \n",
    "Je t'ai demandé si j'étai dans ton coeur, Tu m'a répondu non.\"\"\"\n",
    "\n",
    "import nltk\n",
    "# nltk.data.path.append('C:/Users/shko8/AppData/Roaming/nltk_data')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "print(tokenizer.tokenize(paragraph_french))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e51044b2-ef38-466d-a681-dcd0a3daa01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요, 여러분.', '만나서 반갑습니다.', '이제 텍스트마이닝 클래스를 시작해봅시다!']\n"
     ]
    }
   ],
   "source": [
    "para_kor = \"안녕하세요, 여러분. 만나서 반갑습니다. 이제 텍스트마이닝 클래스를 시작해봅시다!\"\n",
    "print(sent_tokenize(para_kor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "928d31d4-28d3-4d9d-8b08-feebb0fff176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13ead45f-70fb-404b-b88d-8807cf298576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'\", 's', 'good', 'to', 'see', 'you', '.', 'Let', \"'\", 's', 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(WordPunctTokenizer().tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98c975b4-465b-4e84-9777-74c761d26221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요', ',', '여러분', '.', '만나서', '반갑습니다', '.', '이제', '텍스트마이닝', '클래스를', '시작해봅시다', '!']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(para_kor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee70511f-8946-4ae4-8821-b13d74aacffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.findall(\"[abc]\", 'how are you, boy?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5eb3819-abe9-4d20-9bc0-44ed5ded6b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '7', '5', '9']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[0123456789]','3a7b5c9d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b8fe688-9804-4ece-96e0-e7eda394ab02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', 'a', '7', 'b', '_', '5', 'c', '9', 'd']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[\\w]\", \"3a 7b_ ',^&5c9d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07b67e0f-1ff8-4fe6-bfbf-e84e9ae61b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_', '__', '__']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[_]+\",\"a_b, c__d, e__f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "139e6d4f-6c56-4b35-a55b-d5a7675d79ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how', 'are', 'you', 'boy']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[\\w]+\",'how are you, boy?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e51434e-db82-4750-b92d-0541b975e4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oo', 'ooo', 'oooo', 'ooo']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[o]{2,4}\", \"oh, hoow are yooou, boooooooy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5461c327-e872-43cd-9181-881e37a49c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', 'I', \"can't\", 'go', 'there']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "print(tokenizer.tokenize(\"sorry, I can't go there\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e62ba74a-daa7-4a1a-9088-66b073326a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', 'I', 'can', 't', 'go', 'there']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
    "print(tokenizer.tokenize(\"sorry, I can't go there\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12710fc9-e440-4a66-b431-0be4511727db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', \"can't\", 'there']\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Sorry, I can't go there\"\n",
    "tokenizer = RegexpTokenizer(\"[\\w']{3,}\")\n",
    "print(tokenizer.tokenize(text1.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0495c5e6-4601-469e-966f-29a2c0440623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', 'go', 'movie', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "text1 = \"Sorry, I couldn't go to movie yesterday\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokens = tokenizer.tokenize(text1.lower())\n",
    "\n",
    "result = [word for word in tokens if word not in english_stops]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1bff2ae6-8742-4567-ae9c-8043f0251887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"you've\", 'in', 'what', 'from', 'your', 'had', 'very', 'only', 'doesn', 'that', 'at', 'her', 'through', 'their', 'with', 've', 'not', 'own', 'there', 'couldn', 'each', 'hers', \"haven't\", 'against', 'out', 'such', 'yourselves', 'when', 's', 'now', 'where', 'how', 'after', 'them', \"needn't\", \"won't\", 'herself', 'do', 'all', 'here', 'further', \"mustn't\", 'are', 'was', 'themselves', 'isn', 'be', 'until', 'up', 'will', 'wouldn', \"couldn't\", 'don', 'below', 'were', 'needn', 'aren', 'its', 'hasn', 'you', 'any', 'why', 'they', 'those', \"you'd\", 'and', 'wasn', 'under', 'about', 'once', 'he', 'a', 'same', \"doesn't\", 'him', \"shouldn't\", 'am', \"hadn't\", 'shan', \"wasn't\", 'of', 'whom', 'doing', 'before', 'most', 't', 'it', 're', 'again', 'just', 'been', 'which', 'm', 'into', \"aren't\", 'ours', 'between', 'won', 'y', 'mustn', \"weren't\", 'my', 'on', 'so', 'should', 'as', 'to', 'ourselves', 'too', \"it's\", 'o', 'having', 'who', 'during', 'yours', 'for', 'we', 'his', 'than', 'both', \"don't\", \"you're\", 'some', \"mightn't\", 'other', 'but', 'off', \"she's\", 'is', 'more', 'the', 'then', 'few', 'over', 'itself', 'ain', 'll', 'have', 'can', 'being', 'haven', 'i', 'myself', \"that'll\", \"didn't\", 'theirs', 'an', 'if', \"shan't\", 'ma', 'shouldn', 'by', 'weren', 'these', 'because', \"hasn't\", \"you'll\", 'himself', 'd', 'yourself', \"wouldn't\", 'mightn', \"should've\", 'does', 'she', 'while', 'did', 'nor', 'has', 'no', 'hadn', 'above', \"isn't\", 'our', 'didn', 'or', 'me', 'this', 'down'}\n"
     ]
    }
   ],
   "source": [
    "print(english_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "98f16e81-1e06-42fd-a658-8777b8b79cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopword = ['i', 'go', 'to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "767bd8e0-f646-4d4d-a979-f6a77826e3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', \"couldn't\", 'movie', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "result = [word for word in tokens if word not in my_stopword]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "937e1880-60e1-4608-9409-3dc59e04faa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookeri cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3c08b45d-7bd6-4d32-a3c9-20da5722d2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n",
      "['hello', 'everyon', '.', 'it', \"'s\", 'good', 'to', 'see', 'you', '.', 'let', \"'s\", 'start', 'our', 'text', 'mine', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
    "tokens = word_tokenize(para)\n",
    "print(tokens)\n",
    "result= [stemmer.stem(token) for token in tokens]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d0a8ba26-10d3-48fd-979d-187e1d97336c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookery cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c39504c-c23b-4270-8f7b-13b6aee60955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking\n",
      "cook\n",
      "cookery\n",
      "cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('cooking'))\n",
    "print(lemmatizer.lemmatize('cooking', pos='v'))\n",
    "print(lemmatizer.lemmatize('cookery'))\n",
    "print(lemmatizer.lemmatize('cookbooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3739fa21-62c8-47eb-a359-5c93e22a19a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming result:  believ\n",
      "lemmatizing result:  belief\n",
      "lemmatizing result:  believe\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "print('stemming result: ', stemmer.stem('believes'))\n",
    "print('lemmatizing result: ', lemmatizer.lemmatize('believes'))\n",
    "print('lemmatizing result: ', lemmatizer.lemmatize('believes', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ebdaa4-a537-4a7e-9032-5b47d410a7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('everyone', 'NN'), ('.', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('good', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('.', '.'), ('Let', 'VB'), (\"'s\", 'POS'), ('start', 'VB'), ('our', 'PRP$'), ('text', 'NN'), ('mining', 'NN'), ('class', 'NN'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(\"Hello everyone. It's good to see you. Let's start our text mining class!\")\n",
    "print(nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "70a85461-5d7c-4b30-87f9-2b23da5f5502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('CC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bd642a9d-ace4-4404-a8eb-19a24441e71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['everyone', 'good', 'see', 'Let', 'start', 'text', 'mining', 'class']\n"
     ]
    }
   ],
   "source": [
    "my_tag_set = ['NN', 'VB', 'JJ']\n",
    "my_words = [word for word, tag in nltk.pos_tag(tokens) if tag in my_tag_set]\n",
    "print(my_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0108b993-3e4f-4700-817f-ca8dbd237c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello/NNP', 'everyone/NN', './.', 'It/PRP', \"'s/VBZ\", 'good/JJ', 'to/TO', 'see/VB', 'you/PRP', './.', 'Let/VB', \"'s/POS\", 'start/VB', 'our/PRP$', 'text/NN', 'mining/NN', 'class/NN', '!/.']\n"
     ]
    }
   ],
   "source": [
    "words_with_tag = ['/'.join(item) for item in nltk.pos_tag(tokens)]\n",
    "print(words_with_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eaafcdc-2011-47b1-b9f8-a6dab8826397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['절망의', '반대가', '희망은', '아니다', '.', '어두운', '밤하늘에', '별이', '빛나듯', '희망은', '절망', '속에', '싹트는', '거지', '만약에', '우리가', '희망함이', '적다면', '그', '누가', '세상을', '비출어줄까', '.', '정희성', ',', '희망', '공부']\n",
      "[('절망의', 'JJ'), ('반대가', 'NNP'), ('희망은', 'NNP'), ('아니다', 'NNP'), ('.', '.'), ('어두운', 'VB'), ('밤하늘에', 'JJ'), ('별이', 'NNP'), ('빛나듯', 'NNP'), ('희망은', 'NNP'), ('절망', 'NNP'), ('속에', 'NNP'), ('싹트는', 'NNP'), ('거지', 'NNP'), ('만약에', 'NNP'), ('우리가', 'NNP'), ('희망함이', 'NNP'), ('적다면', 'NNP'), ('그', 'NNP'), ('누가', 'NNP'), ('세상을', 'NNP'), ('비출어줄까', 'NNP'), ('.', '.'), ('정희성', 'NN'), (',', ','), ('희망', 'NNP'), ('공부', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "sentence = '''절망의 반대가 희망은 아니다.\n",
    "어두운 밤하늘에 별이 빛나듯\n",
    "희망은 절망 속에 싹트는 거지\n",
    "만약에 우리가 희망함이 적다면\n",
    "그 누가 세상을 비출어줄까.\n",
    "정희성, 희망 공부'''\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n",
    "print(nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1673bc23-84ca-4861-97ef-9ec0cf5bde6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from konlpy) (1.5.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from konlpy) (5.3.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from konlpy) (1.24.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\shko8\\.conda\\envs\\py3_8_5\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ed014f2-dea2-4650-8a79-22efb2fdbcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "t = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbdd14cf-8303-41f8-aea5-039c9c725de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소:  ['절망', '의', '반대', '가', '희망', '은', '아니다', '.', '\\n', '어', '두운', '밤하늘', '에', '별', '이', '빛나듯', '\\n', '희망', '은', '절망', '속', '에', '싹트는', '거지', '\\n', '만약', '에', '우리', '가', '희망', '함', '이', '적다면', '\\n', '그', '누가', '세상', '을', '비출어줄까', '.', '\\n', '정희성', ',', '희망', '공부']\n",
      "\n",
      "명사 ['절망', '반대', '희망', '어', '두운', '밤하늘', '별', '희망', '절망', '속', '거지', '만약', '우리', '희망', '함', '그', '누가', '세상', '정희성', '희망', '공부']\n",
      "\n",
      "품사 태깅 결과:  [('절망', 'Noun'), ('의', 'Josa'), ('반대', 'Noun'), ('가', 'Josa'), ('희망', 'Noun'), ('은', 'Josa'), ('아니다', 'Adjective'), ('.', 'Punctuation'), ('\\n', 'Foreign'), ('어', 'Noun'), ('두운', 'Noun'), ('밤하늘', 'Noun'), ('에', 'Josa'), ('별', 'Noun'), ('이', 'Josa'), ('빛나듯', 'Verb'), ('\\n', 'Foreign'), ('희망', 'Noun'), ('은', 'Josa'), ('절망', 'Noun'), ('속', 'Noun'), ('에', 'Josa'), ('싹트는', 'Verb'), ('거지', 'Noun'), ('\\n', 'Foreign'), ('만약', 'Noun'), ('에', 'Josa'), ('우리', 'Noun'), ('가', 'Josa'), ('희망', 'Noun'), ('함', 'Noun'), ('이', 'Josa'), ('적다면', 'Verb'), ('\\n', 'Foreign'), ('그', 'Noun'), ('누가', 'Noun'), ('세상', 'Noun'), ('을', 'Josa'), ('비출어줄까', 'Verb'), ('.', 'Punctuation'), ('\\n', 'Foreign'), ('정희성', 'Noun'), (',', 'Punctuation'), ('희망', 'Noun'), ('공부', 'Noun')]\n"
     ]
    }
   ],
   "source": [
    "print('형태소: ', t.morphs(sentence))\n",
    "print()\n",
    "print('명사', t.nouns(sentence))\n",
    "print()\n",
    "print('품사 태깅 결과: ', t.pos(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5decc126-b0d5-4462-bd79-3215abfb91b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03576738-864d-42cb-9b7d-7a4cc4bb1c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdf41c1-235b-40f5-9d14-25394e99265c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf57226d-88bc-44b3-b72f-a041e0f1e437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_8_5",
   "language": "python",
   "name": "py3_8_5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
